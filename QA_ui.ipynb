{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the USER_AGENT environment variable\n",
    "os.environ['USER_AGENT'] = 'PDF_QA/1.0'\n",
    "\n",
    "# Rest of your imports and code\n",
    "import gradio as gr\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community import embeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import ollama\n",
    "##############\n",
    "from langchain.document_loaders import DirectoryLoader, UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "##############\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(pdfs, question):\n",
    "    model_local = ChatOllama(model=\"mistral\")\n",
    "    \n",
    "    # Load PDFs\n",
    "    docs = [PyPDFLoader(pdf.name).load() for pdf in pdfs]\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "    \n",
    "    # Split text\n",
    "    # Optimized Chunking Strategy\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=7500,         # Smaller chunk size for better context\n",
    "    chunk_overlap=100,       # Overlap to ensure context retention\n",
    "    )\n",
    "\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "    #Add to vector database\n",
    "    vector_db = Chroma.from_documents(  # noqa: F841\n",
    "        documents = doc_splits,\n",
    "        embedding = OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True),\n",
    "        collection_name=\"rag-chroma\"\n",
    "    )\n",
    "\n",
    "    retriever = vector_db.as_retriever()\n",
    "\n",
    "    # Define the prompt template\n",
    "    after_rag_template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    after_rag_prompt = ChatPromptTemplate.from_template(after_rag_template)\n",
    "    after_rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | after_rag_prompt\n",
    "        | model_local\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Run the chain and return the result\n",
    "    return after_rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gradio interface\n",
    "iface = gr.Interface(fn=process_input,\n",
    "                     inputs=[gr.File(label=\"Upload PDF files\", file_count=\"multiple\", file_types=[\"pdf\"]),\n",
    "                             gr.Textbox(label=\"Question\")],\n",
    "                     outputs=\"text\",\n",
    "                     title=\"PDF Query with Ollama\",\n",
    "                     description=\"Upload PDF files and enter a question to query the documents.\")\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the USER_AGENT environment variable\n",
    "os.environ['USER_AGENT'] = 'DocumentQA/1.0'\n",
    "import gradio as gr\n",
    "from langchain_community.document_loaders import PyPDFLoader, OnlinePDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        self.docs_list = []\n",
    "        self.vector_db = None\n",
    "\n",
    "    def load_documents(self, files):\n",
    "        loaders = {\n",
    "            'pdf': PyPDFLoader,\n",
    "            'txt': self._load_text_file,\n",
    "            'docx': OnlinePDFLoader,  # Assuming you have a specific loader for docx files\n",
    "            # Add more file type loaders here if needed\n",
    "        }\n",
    "\n",
    "        for file in files:\n",
    "            ext = file.name.split('.')[-1].lower()\n",
    "            loader = loaders.get(ext)\n",
    "            if loader:\n",
    "                if ext == 'txt':\n",
    "                    docs = loader(file.name)\n",
    "                else:\n",
    "                    docs = loader(file.name).load()\n",
    "                \n",
    "                # Ensure docs are in the expected format\n",
    "                for doc in docs:\n",
    "                    if isinstance(doc, str):\n",
    "                        self.docs_list.append({\"page_content\": doc, \"metadata\": {\"source\": file.name}})\n",
    "                    else:\n",
    "                        self.docs_list.append(doc)\n",
    "\n",
    "    def _load_text_file(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        # Return in the expected document format\n",
    "        return [{\"page_content\": text, \"metadata\": {\"source\": file_path}}]\n",
    "\n",
    "    def process_documents(self):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=7500,         # Smaller chunk size for better context\n",
    "            chunk_overlap=100,       # Overlap to ensure context retention\n",
    "        )\n",
    "        doc_splits = text_splitter.split_documents(self.docs_list)\n",
    "\n",
    "        # Create the vectorstore only once\n",
    "        self.vector_db = Chroma.from_documents(\n",
    "            documents=doc_splits,\n",
    "            embedding=OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True),\n",
    "            collection_name=\"rag-chroma\"\n",
    "        )\n",
    "\n",
    "    def get_retriever(self):\n",
    "        if self.vector_db is None:\n",
    "            raise ValueError(\"Documents have not been processed. Call process_documents() first.\")\n",
    "        return self.vector_db.as_retriever()\n",
    "\n",
    "class QuestionAnsweringSystem:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.model_local = ChatOllama(model=\"mistral\")\n",
    "        self.prompt_template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "    \n",
    "    def load_and_process(self, files):\n",
    "        self.processor.load_documents(files)\n",
    "        self.processor.process_documents()\n",
    "\n",
    "    def answer_question(self, question):\n",
    "        retriever = self.processor.get_retriever()\n",
    "        after_rag_prompt = ChatPromptTemplate.from_template(self.prompt_template)\n",
    "        after_rag_chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | after_rag_prompt\n",
    "            | self.model_local\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        return after_rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gradio interface\n",
    "def process_input(files, question):\n",
    "    qa_system = QuestionAnsweringSystem()\n",
    "    qa_system.load_and_process(files)\n",
    "    return qa_system.answer_question(question)\n",
    "\n",
    "iface = gr.Interface(fn=process_input,\n",
    "                     inputs=[gr.File(label=\"Upload Documents\", file_count=\"multiple\", file_types=[\"pdf\", \"txt\", \"docx\"]),\n",
    "                             gr.Textbox(label=\"Question\")],\n",
    "                     outputs=\"text\",\n",
    "                     title=\"Document Query with Ollama\",\n",
    "                     description=\"Upload documents in various formats and enter a question to query the content.\")\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working perfectly fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the USER_AGENT environment variable\n",
    "os.environ['USER_AGENT'] = 'DocumentQA/1.0'\n",
    "import gradio as gr\n",
    "from langchain_community.document_loaders import PyPDFLoader, OnlinePDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        self.docs_list = []\n",
    "        self.vector_db = None\n",
    "\n",
    "    def load_documents(self, files):\n",
    "        loaders = {\n",
    "            'pdf': PyPDFLoader,\n",
    "            'txt': self._load_text_file,\n",
    "            'docx': OnlinePDFLoader,  # Assuming you have a specific loader for docx files\n",
    "            # Add more file type loaders here if needed\n",
    "        }\n",
    "\n",
    "        for file in files:\n",
    "            ext = file.name.split('.')[-1].lower()\n",
    "            loader = loaders.get(ext)\n",
    "            if loader:\n",
    "                if ext == 'txt':\n",
    "                    docs = loader(file.name)\n",
    "                else:\n",
    "                    docs = loader(file.name).load()\n",
    "\n",
    "                # Ensure docs are in the expected format\n",
    "                for doc in docs:\n",
    "                    if isinstance(doc, Document):\n",
    "                        self.docs_list.append(doc)\n",
    "                    else:\n",
    "                        self.docs_list.append(Document(page_content=doc.get('page_content', doc), metadata=doc.get('metadata', {\"source\": file.name})))\n",
    "\n",
    "    def _load_text_file(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        # Return in the expected document format\n",
    "        return [Document(page_content=text, metadata={\"source\": file_path})]\n",
    "\n",
    "    def process_documents(self):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=7500,         # Smaller chunk size for better context\n",
    "            chunk_overlap=100,       # Overlap to ensure context retention\n",
    "        )\n",
    "        doc_splits = text_splitter.split_documents(self.docs_list)\n",
    "\n",
    "        # Create the vectorstore only once\n",
    "        self.vector_db = Chroma.from_documents(\n",
    "            documents=doc_splits,\n",
    "            embedding=OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True),\n",
    "            collection_name=\"rag-chroma\"\n",
    "        )\n",
    "\n",
    "    def get_retriever(self):\n",
    "        if self.vector_db is None:\n",
    "            raise ValueError(\"Documents have not been processed. Call process_documents() first.\")\n",
    "        return self.vector_db.as_retriever()\n",
    "\n",
    "class QuestionAnsweringSystem:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.model_local = ChatOllama(model=\"mistral\")\n",
    "        self.prompt_template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "    \n",
    "    def load_and_process(self, files):\n",
    "        self.processor.load_documents(files)\n",
    "        self.processor.process_documents()\n",
    "\n",
    "    def answer_question(self, question):\n",
    "        retriever = self.processor.get_retriever()\n",
    "        after_rag_prompt = ChatPromptTemplate.from_template(self.prompt_template)\n",
    "        after_rag_chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | after_rag_prompt\n",
    "            | self.model_local\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        return after_rag_chain.invoke(question)\n",
    "\n",
    "# Define Gradio interface\n",
    "def process_input(files, question):\n",
    "    qa_system = QuestionAnsweringSystem()\n",
    "    qa_system.load_and_process(files)\n",
    "    return qa_system.answer_question(question)\n",
    "\n",
    "iface = gr.Interface(fn=process_input,\n",
    "                     inputs=[gr.File(label=\"Upload Documents\", file_count=\"multiple\", file_types=[\"pdf\", \"txt\", \"docx\"]),\n",
    "                             gr.Textbox(label=\"Question\")],\n",
    "                     outputs=\"text\",\n",
    "                     title=\"Document Query with Ollama\",\n",
    "                     description=\"Upload documents in various formats and enter a question to query the content.\")\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the USER_AGENT environment variable\n",
    "os.environ['USER_AGENT'] = 'DocumentQA/1.0'\n",
    "import gradio as gr\n",
    "from langchain_community.document_loaders import PyPDFLoader, OnlinePDFLoader\n",
    "from langchain_chroma import Chroma  # Updated import\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, db_directory=\"vector_db\"):\n",
    "        # Initialize or load the vector database from disk\n",
    "        self.embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)\n",
    "        self.vector_db = Chroma(persist_directory=db_directory,\n",
    "                                embedding_function=self.embeddings,\n",
    "                                collection_name=\"rag-chroma\")\n",
    "\n",
    "    def load_documents(self, files):\n",
    "        docs_list = []\n",
    "        loaders = {\n",
    "            'pdf': PyPDFLoader,\n",
    "            'txt': self._load_text_file,\n",
    "            'docx': OnlinePDFLoader,  # Assuming you have a specific loader for docx files\n",
    "            # Add more file type loaders here if needed\n",
    "        }\n",
    "\n",
    "        for file in files:\n",
    "            ext = file.name.split('.')[-1].lower()\n",
    "            loader = loaders.get(ext)\n",
    "            if loader:\n",
    "                if ext == 'txt':\n",
    "                    docs = loader(file.name)\n",
    "                else:\n",
    "                    docs = loader(file.name).load()\n",
    "\n",
    "                # Ensure docs are in the expected format\n",
    "                for doc in docs:\n",
    "                    if isinstance(doc, Document):\n",
    "                        docs_list.append(doc)\n",
    "                    else:\n",
    "                        docs_list.append(Document(page_content=doc.get('page_content', doc), metadata=doc.get('metadata', {\"source\": file.name})))\n",
    "\n",
    "        # Process and add the documents to the vector store\n",
    "        self._process_and_store_documents(docs_list)\n",
    "\n",
    "    def _load_text_file(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        return [Document(page_content=text, metadata={\"source\": file_path})]\n",
    "\n",
    "    def _process_and_store_documents(self, docs_list):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=7500,\n",
    "            chunk_overlap=100,\n",
    "        )\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "        # Add documents to the vector store\n",
    "        self.vector_db.add_documents(doc_splits)\n",
    "        # No need to explicitly persist; the vector store should handle it automatically\n",
    "\n",
    "    def get_retriever(self):\n",
    "        return self.vector_db.as_retriever()\n",
    "\n",
    "class QuestionAnsweringSystem:\n",
    "    def __init__(self):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.model_local = ChatOllama(model=\"mistral\")\n",
    "        self.prompt_template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "\n",
    "    def load_and_process(self, files):\n",
    "        self.processor.load_documents(files)\n",
    "\n",
    "    def answer_question(self, question):\n",
    "        retriever = self.processor.get_retriever()\n",
    "        after_rag_prompt = ChatPromptTemplate.from_template(self.prompt_template)\n",
    "        after_rag_chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | after_rag_prompt\n",
    "            | self.model_local\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        return after_rag_chain.invoke(question)\n",
    "\n",
    "# Gradio Interfaces for Uploading Documents and Asking Questions\n",
    "def upload_documents(files):\n",
    "    qa_system = QuestionAnsweringSystem()\n",
    "    qa_system.load_and_process(files)\n",
    "    return \"Documents uploaded and processed successfully!\"\n",
    "\n",
    "def query_documents(question):\n",
    "    qa_system = QuestionAnsweringSystem()\n",
    "    return qa_system.answer_question(question)\n",
    "\n",
    "iface = gr.Blocks()\n",
    "\n",
    "with iface:\n",
    "    with gr.Tab(\"Upload Documents\"):\n",
    "        file_input = gr.File(label=\"Upload Documents\", file_count=\"multiple\", file_types=[\"pdf\", \"txt\", \"docx\"])\n",
    "        upload_button = gr.Button(\"Upload\")\n",
    "        upload_output = gr.Textbox(label=\"Upload Status\")\n",
    "        upload_button.click(upload_documents, inputs=file_input, outputs=upload_output)\n",
    "    \n",
    "    with gr.Tab(\"Query Documents\"):\n",
    "        question_input = gr.Textbox(label=\"Question\")\n",
    "        query_button = gr.Button(\"Ask\")\n",
    "        query_output = gr.Textbox(label=\"Answer\")\n",
    "        query_button.click(query_documents, inputs=question_input, outputs=query_output)\n",
    "\n",
    "iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
